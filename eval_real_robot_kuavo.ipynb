{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///\n",
      "\u001b[31mERROR: file:/// does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"/app/data/outputs/2024.09.22/07.35.36_train_diffusion_unet_hybrid_pusht_image/checkpoints/latest.ckpt\"\n",
    "output=\"/app/data/outputs/2024.09.22/07.35.36_train_diffusion_unet_hybrid_pusht_image/checkpoints/output\"\n",
    "robot_ip=\"192.168.0.204\"\n",
    "match_dataset=\"/app/data/pusht_real/real_pusht_20230105\"\n",
    "match_episode=None\n",
    "vis_camera_idx=0\n",
    "init_joints=False\n",
    "steps_per_inference=6\n",
    "max_duration=60\n",
    "frequency=10\n",
    "command_latency=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/opt/ros/noetic/lib/python3/dist-packages\")\n",
    "sys.path.append(\"/app/diffusion_policy/real_world/dynamic_biped_ws/devel/lib/python3/dist-packages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source /app/diffusion_policy/real_world/dynamic_biped_ws/devel/setup.zsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospkg\n",
    "import rospy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/robodiff/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from multiprocessing.managers import SharedMemoryManager\n",
    "import click\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import dill\n",
    "import hydra\n",
    "import pathlib\n",
    "import skvideo.io\n",
    "from omegaconf import OmegaConf\n",
    "import scipy.spatial.transform as st\n",
    "from diffusion_policy.real_world.real_env_kuavo import KuavoEnv\n",
    "# from diffusion_policy.real_world.spacemouse_shared_memory import Spacemouse\n",
    "from diffusion_policy.common.precise_sleep import precise_wait\n",
    "from diffusion_policy.real_world.real_inference_util import (\n",
    "    get_real_obs_resolution, \n",
    "    get_real_obs_dict)\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.policy.base_image_policy import BaseImagePolicy\n",
    "from diffusion_policy.common.cv2_util import get_image_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded initial frame for 1 episodes\n"
     ]
    }
   ],
   "source": [
    "# load match_dataset\n",
    "match_camera_idx = 0\n",
    "episode_first_frame_map = dict()\n",
    "if match_dataset is not None:\n",
    "    match_dir = pathlib.Path(match_dataset)\n",
    "    match_video_dir = match_dir.joinpath('videos')\n",
    "    for vid_dir in match_video_dir.glob(\"*/\"):\n",
    "        episode_idx = int(vid_dir.stem)\n",
    "        match_video_path = vid_dir.joinpath(f'{match_camera_idx}.mp4')\n",
    "        if match_video_path.exists():\n",
    "            frames = skvideo.io.vread(\n",
    "                str(match_video_path), num_frames=1)\n",
    "            episode_first_frame_map[episode_idx] = frames[0]\n",
    "        break\n",
    "print(f\"Loaded initial frame for {len(episode_first_frame_map)} episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://172.17.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://172.17.0.1:7890'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['agent_pos']\n",
      "using obs modality: rgb with keys: ['image']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n",
      "Diffusion params: 6.502567e+07\n",
      "Vision params: 1.119709e+07\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "ckpt_path = input\n",
    "payload = torch.load(open(ckpt_path, 'rb'), pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "cls = hydra.utils.get_class(cfg._target_)\n",
    "workspace = cls(cfg)\n",
    "workspace: BaseWorkspace\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacks for method-specific setup.\n",
    "action_offset = 0\n",
    "delta_action = False\n",
    "if 'diffusion' in cfg.name:\n",
    "    # diffusion model\n",
    "    policy: BaseImagePolicy\n",
    "    policy = workspace.model\n",
    "    if cfg.training.use_ema:\n",
    "        policy = workspace.ema_model\n",
    "    device = torch.device('cuda')\n",
    "    policy.eval().to(device)\n",
    "    # set inference params\n",
    "    policy.num_inference_steps = 16 # DDIM inference iterations\n",
    "    policy.n_action_steps = 32\n",
    "elif 'robomimic' in cfg.name:\n",
    "    # BCRNN model\n",
    "    policy: BaseImagePolicy\n",
    "    policy = workspace.model\n",
    "    device = torch.device('cuda')\n",
    "    policy.eval().to(device)\n",
    "    # BCRNN always has action horizon of 1\n",
    "    steps_per_inference = 1\n",
    "    action_offset = cfg.n_latency_steps\n",
    "    delta_action = cfg.task.dataset.get('delta_action', False)\n",
    "elif 'ibc' in cfg.name:\n",
    "    policy: BaseImagePolicy\n",
    "    policy = workspace.model\n",
    "    policy.pred_n_iter = 5\n",
    "    policy.pred_n_samples = 4096\n",
    "    device = torch.device('cuda')\n",
    "    policy.eval().to(device)\n",
    "    steps_per_inference = 1\n",
    "    action_offset = 1\n",
    "    delta_action = cfg.task.dataset.get('delta_action', False)\n",
    "else:\n",
    "    raise RuntimeError(\"Unsupported policy type: \", cfg.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_target_': 'diffusion_policy.workspace.train_diffusion_unet_hybrid_workspace.TrainDiffusionUnetHybridWorkspace', 'checkpoint': {'save_last_ckpt': True, 'save_last_snapshot': False, 'topk': {'format_str': 'epoch={epoch:04d}-train_loss={train_loss:.3f}.ckpt', 'k': 5, 'mode': 'min', 'monitor_key': 'train_loss'}}, 'dataloader': {'batch_size': 128, 'num_workers': 8, 'persistent_workers': False, 'pin_memory': True, 'shuffle': True}, 'dataset_obs_steps': 2, 'ema': {'_target_': 'diffusion_policy.model.diffusion.ema_model.EMAModel', 'inv_gamma': 1.0, 'max_value': 0.9999, 'min_value': 0.0, 'power': 0.75, 'update_after_step': 0}, 'exp_name': 'default', 'horizon': 16, 'keypoint_visible_rate': 1.0, 'logging': {'group': None, 'id': None, 'mode': 'online', 'name': '2023.01.16-20.20.06_train_diffusion_unet_hybrid_pusht_image', 'project': 'diffusion_policy_debug', 'resume': True, 'tags': ['train_diffusion_unet_hybrid', 'pusht_image', 'default']}, 'multi_run': {'run_dir': 'data/outputs/2023.01.16/20.20.06_train_diffusion_unet_hybrid_pusht_image', 'wandb_name_base': '2023.01.16-20.20.06_train_diffusion_unet_hybrid_pusht_image'}, 'n_action_steps': 8, 'n_latency_steps': 0, 'n_obs_steps': 2, 'name': 'train_diffusion_unet_hybrid', 'obs_as_global_cond': True, 'optimizer': {'_target_': 'torch.optim.AdamW', 'betas': [0.95, 0.999], 'eps': 1e-08, 'lr': 0.0001, 'weight_decay': 1e-06}, 'past_action_visible': False, 'policy': {'_target_': 'diffusion_policy.policy.diffusion_unet_hybrid_image_policy.DiffusionUnetHybridImagePolicy', 'cond_predict_scale': True, 'crop_shape': [84, 84], 'diffusion_step_embed_dim': 128, 'down_dims': [256, 512, 1024], 'eval_fixed_crop': True, 'horizon': 16, 'kernel_size': 5, 'n_action_steps': 8, 'n_groups': 8, 'n_obs_steps': 2, 'noise_scheduler': {'_target_': 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler', 'beta_end': 0.02, 'beta_schedule': 'squaredcos_cap_v2', 'beta_start': 0.0001, 'clip_sample': True, 'num_train_timesteps': 100, 'prediction_type': 'epsilon', 'variance_type': 'fixed_small'}, 'num_inference_steps': 100, 'obs_as_global_cond': True, 'obs_encoder_group_norm': True, 'shape_meta': {'action': {'shape': [7]}, 'obs': {'agent_pos': {'shape': [7], 'type': 'low_dim'}, 'image': {'shape': [3, 96, 96], 'type': 'rgb'}}}}, 'shape_meta': {'action': {'shape': [7]}, 'obs': {'agent_pos': {'shape': [7], 'type': 'low_dim'}, 'image': {'shape': [3, 96, 96], 'type': 'rgb'}}}, 'task': {'dataset': {'_target_': 'diffusion_policy.dataset.pusht_image_dataset_1.PushTImageDataset', 'horizon': 16, 'max_train_episodes': 90, 'pad_after': 7, 'pad_before': 1, 'seed': 42, 'val_ratio': 0.02, 'zarr_path': 'data/BJ_juice1/zarr/BJ_juice1.zarr'}, 'env_runner': {'_target_': 'diffusion_policy.env_runner.real_pusht_image_runner.RealPushTImageRunner'}, 'image_shape': [3, 96, 96], 'name': 'pusht_image', 'shape_meta': {'action': {'shape': [7]}, 'obs': {'agent_pos': {'shape': [7], 'type': 'low_dim'}, 'image': {'shape': [3, 96, 96], 'type': 'rgb'}}}}, 'task_name': 'pusht_image', 'training': {'checkpoint_every': 50, 'debug': False, 'device': 'cuda:0', 'gradient_accumulate_every': 1, 'lr_scheduler': 'cosine', 'lr_warmup_steps': 500, 'max_train_steps': None, 'max_val_steps': None, 'num_epochs': 10, 'resume': True, 'rollout_every': 50, 'sample_every': 5, 'seed': 42, 'tqdm_interval_sec': 1.0, 'use_ema': True, 'val_every': 10}, 'val_dataloader': {'batch_size': 128, 'num_workers': 8, 'persistent_workers': False, 'pin_memory': True, 'shuffle': False}}\n",
      "n_obs_steps:  2\n",
      "steps_per_inference: 6\n",
      "action_offset: 0\n",
      "n_action_steps 8\n"
     ]
    }
   ],
   "source": [
    "# setup experiment\n",
    "dt = 1/frequency\n",
    "print(cfg)\n",
    "obs_res = get_real_obs_resolution(cfg.task.shape_meta)\n",
    "n_obs_steps = cfg.n_obs_steps\n",
    "print(\"n_obs_steps: \", n_obs_steps)\n",
    "print(\"steps_per_inference:\", steps_per_inference)\n",
    "print(\"action_offset:\", action_offset)\n",
    "print('n_action_steps', cfg.n_action_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KuavoEnv(\n",
    "        output_dir=output,\n",
    "        frequency=frequency,\n",
    "        n_obs_steps=2,\n",
    "        obs_image_resolution=(640, 480),\n",
    "        max_obs_buffer_size=30,\n",
    "        robot_publish_rate=125,\n",
    "        video_capture_fps=30,\n",
    "        video_capture_resolution=(1280, 720),\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image (2, 480, 640, 3)\n",
      "agent_pos (2, 7)\n",
      "timestamp (2,)\n"
     ]
    }
   ],
   "source": [
    "fake_obs=env.get_fake_obs()\n",
    "for k, v in fake_obs.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!\n",
      "get_obs\n",
      "Obs latency -0.08899044990539551\n",
      "1 32\n",
      "Inference latency: 0.10473346710205078\n",
      "(15, 7)\n",
      "executing actions: (15, 7)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # start episode\n",
    "    policy.reset()\n",
    "    start_delay = 1.0\n",
    "    eval_t_start = time.time() + start_delay\n",
    "    t_start = time.monotonic() + start_delay\n",
    "    # env.start_episode(eval_t_start)\n",
    "    # wait for 1/30 sec to get the closest frame actually\n",
    "    # reduces overall latency\n",
    "    frame_latency = 1/30\n",
    "    precise_wait(eval_t_start - frame_latency, time_func=time.time)\n",
    "    print(\"Started!\")\n",
    "    iter_idx = 0\n",
    "    term_area_start_timestamp = float('inf')\n",
    "    perv_target_pose = None\n",
    "    while True:\n",
    "        # calculate timing\n",
    "        t_cycle_end = t_start + (iter_idx + steps_per_inference) * dt\n",
    "        # get obs\n",
    "        print('get_obs')\n",
    "        # obs = env.get_obs()\n",
    "        obs = env.get_fake_obs()\n",
    "        obs_timestamps = obs['timestamp']\n",
    "        print(f'Obs latency {time.time() - obs_timestamps[-1]}')\n",
    "        # run inference\n",
    "        with torch.no_grad():\n",
    "            s = time.time()\n",
    "            obs_dict_np = get_real_obs_dict(\n",
    "                env_obs=obs, shape_meta=cfg.task.shape_meta)\n",
    "            obs_dict = dict_apply(obs_dict_np, \n",
    "                lambda x: torch.from_numpy(x).unsqueeze(0).to(device))\n",
    "            result = policy.predict_action(obs_dict)\n",
    "            # this action starts from the first obs step\n",
    "            action = result['action'][0].detach().to('cpu').numpy()\n",
    "            print('Inference latency:', time.time() - s)\n",
    "        \n",
    "        # # clip actions\n",
    "        # this_target_poses[:,:2] = np.clip(\n",
    "        #     this_target_poses[:,:2], [0.25, -0.45], [0.77, 0.40])\n",
    "        # execute actions\n",
    "        print(action.shape)\n",
    "        env.exec_fake_actions(\n",
    "            actions=action,\n",
    "        )\n",
    "        break\n",
    "        \n",
    "        # print(f\"Submitted {len(action)} steps of actions.\")\n",
    "        # # visualize\n",
    "        # episode_id = env.replay_buffer.n_episodes\n",
    "        # vis_img = obs[f'camera_{vis_camera_idx}'][-1]\n",
    "        # text = 'Episode: {}, Time: {:.1f}'.format(\n",
    "        #     episode_id, time.monotonic() - t_start\n",
    "        # )\n",
    "        # cv2.putText(\n",
    "        #     vis_img,\n",
    "        #     text,\n",
    "        #     (10,20),\n",
    "        #     fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        #     fontScale=0.5,\n",
    "        #     thickness=1,\n",
    "        #     color=(255,255,255)\n",
    "        # )\n",
    "        # cv2.imshow('default', vis_img[...,::-1])\n",
    "        # key_stroke = cv2.pollKey()\n",
    "        # if key_stroke == ord('s'):\n",
    "        #     # Stop episode\n",
    "        #     # Hand control back to human\n",
    "        #     env.end_episode()\n",
    "        #     print('Stopped.')\n",
    "        #     break\n",
    "        # # auto termination\n",
    "        # terminate = False\n",
    "        # if time.monotonic() - t_start > max_duration:\n",
    "        #     terminate = True\n",
    "        #     print('Terminated by the timeout!')\n",
    "        # term_pose = np.array([ 3.40948500e-01,  2.17721816e-01,  4.59076878e-02,  2.22014183e+00, -2.22184883e+00, -4.07186655e-04])\n",
    "        # curr_pose = obs['robot_eef_pose'][-1]\n",
    "        # dist = np.linalg.norm((curr_pose - term_pose)[:2], axis=-1)\n",
    "        # if dist < 0.03:\n",
    "        #     # in termination area\n",
    "        #     curr_timestamp = obs['timestamp'][-1]\n",
    "        #     if term_area_start_timestamp > curr_timestamp:\n",
    "        #         term_area_start_timestamp = curr_timestamp\n",
    "        #     else:\n",
    "        #         term_area_time = curr_timestamp - term_area_start_timestamp\n",
    "        #         if term_area_time > 0.5:\n",
    "        #             terminate = True\n",
    "        #             print('Terminated by the policy!')\n",
    "        # else:\n",
    "        #     # out of the area\n",
    "        #     term_area_start_timestamp = float('inf')\n",
    "        # if terminate:\n",
    "        #     env.end_episode()\n",
    "        #     break\n",
    "        # # wait for execution\n",
    "        # precise_wait(t_cycle_end - frame_latency)\n",
    "        # iter_idx += steps_per_inference\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted!\")\n",
    "    # stop robot.\n",
    "    env.end_episode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
