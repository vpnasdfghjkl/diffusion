{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///\n",
      "\u001b[31mERROR: file:/// does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"/app/data/outputs/2024.09.22/07.35.36_train_diffusion_unet_hybrid_pusht_image/checkpoints/latest.ckpt\"\n",
    "output=\"/app/data/outputs/2024.09.22/07.35.36_train_diffusion_unet_hybrid_pusht_image/checkpoints/output\"\n",
    "robot_ip=\"192.168.0.204\"\n",
    "match_dataset=\"/app/data/pusht_real/real_pusht_20230105\"\n",
    "match_episode=None\n",
    "vis_camera_idx=0\n",
    "init_joints=False\n",
    "steps_per_inference=6\n",
    "max_duration=60\n",
    "frequency=10\n",
    "command_latency=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/robodiff/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from multiprocessing.managers import SharedMemoryManager\n",
    "import click\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import dill\n",
    "import hydra\n",
    "import pathlib\n",
    "import skvideo.io\n",
    "from omegaconf import OmegaConf\n",
    "import scipy.spatial.transform as st\n",
    "# from diffusion_policy.real_world.real_env import RealEnv\n",
    "# from diffusion_policy.real_world.spacemouse_shared_memory import Spacemouse\n",
    "from diffusion_policy.common.precise_sleep import precise_wait\n",
    "from diffusion_policy.real_world.real_inference_util import (\n",
    "    get_real_obs_resolution, \n",
    "    get_real_obs_dict)\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.policy.base_image_policy import BaseImagePolicy\n",
    "from diffusion_policy.common.cv2_util import get_image_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded initial frame for 1 episodes\n"
     ]
    }
   ],
   "source": [
    "# load match_dataset\n",
    "match_camera_idx = 0\n",
    "episode_first_frame_map = dict()\n",
    "if match_dataset is not None:\n",
    "    match_dir = pathlib.Path(match_dataset)\n",
    "    match_video_dir = match_dir.joinpath('videos')\n",
    "    for vid_dir in match_video_dir.glob(\"*/\"):\n",
    "        episode_idx = int(vid_dir.stem)\n",
    "        match_video_path = vid_dir.joinpath(f'{match_camera_idx}.mp4')\n",
    "        if match_video_path.exists():\n",
    "            frames = skvideo.io.vread(\n",
    "                str(match_video_path), num_frames=1)\n",
    "            episode_first_frame_map[episode_idx] = frames[0]\n",
    "        break\n",
    "print(f\"Loaded initial frame for {len(episode_first_frame_map)} episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['agent_pos']\n",
      "using obs modality: rgb with keys: ['image']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/robodiff/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/robodiff/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusion params: 6.502567e+07\n",
      "Vision params: 1.119709e+07\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "ckpt_path = input\n",
    "payload = torch.load(open(ckpt_path, 'rb'), pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "cls = hydra.utils.get_class(cfg._target_)\n",
    "workspace = cls(cfg)\n",
    "workspace: BaseWorkspace\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacks for method-specific setup.\n",
    "action_offset = 0\n",
    "delta_action = False\n",
    "if 'diffusion' in cfg.name:\n",
    "    # diffusion model\n",
    "    policy: BaseImagePolicy\n",
    "    policy = workspace.model\n",
    "    if cfg.training.use_ema:\n",
    "        policy = workspace.ema_model\n",
    "    device = torch.device('cuda')\n",
    "    policy.eval().to(device)\n",
    "    # set inference params\n",
    "    policy.num_inference_steps = 16 # DDIM inference iterations\n",
    "    policy.n_action_steps = policy.horizon - policy.n_obs_steps + 1\n",
    "elif 'robomimic' in cfg.name:\n",
    "    # BCRNN model\n",
    "    policy: BaseImagePolicy\n",
    "    policy = workspace.model\n",
    "    device = torch.device('cuda')\n",
    "    policy.eval().to(device)\n",
    "    # BCRNN always has action horizon of 1\n",
    "    steps_per_inference = 1\n",
    "    action_offset = cfg.n_latency_steps\n",
    "    delta_action = cfg.task.dataset.get('delta_action', False)\n",
    "elif 'ibc' in cfg.name:\n",
    "    policy: BaseImagePolicy\n",
    "    policy = workspace.model\n",
    "    policy.pred_n_iter = 5\n",
    "    policy.pred_n_samples = 4096\n",
    "    device = torch.device('cuda')\n",
    "    policy.eval().to(device)\n",
    "    steps_per_inference = 1\n",
    "    action_offset = 1\n",
    "    delta_action = cfg.task.dataset.get('delta_action', False)\n",
    "else:\n",
    "    raise RuntimeError(\"Unsupported policy type: \", cfg.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_obs_steps:  2\n",
      "steps_per_inference: 6\n",
      "action_offset: 0\n"
     ]
    }
   ],
   "source": [
    "# setup experiment\n",
    "dt = 1/frequency\n",
    "\n",
    "obs_res = get_real_obs_resolution(cfg.task.shape_meta)\n",
    "n_obs_steps = cfg.n_obs_steps\n",
    "print(\"n_obs_steps: \", n_obs_steps)\n",
    "print(\"steps_per_inference:\", steps_per_inference)\n",
    "print(\"action_offset:\", action_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Callable, Tuple\n",
    "import numpy as np\n",
    "from diffusion_policy.common.cv2_util import get_image_transform\n",
    "\n",
    "def get_real_obs_dict(\n",
    "        env_obs: Dict[str, np.ndarray], \n",
    "        shape_meta: dict,\n",
    "        ) -> Dict[str, np.ndarray]:\n",
    "    obs_dict_np = dict()\n",
    "    obs_shape_meta = shape_meta['obs']\n",
    "    for key, attr in obs_shape_meta.items():\n",
    "        type = attr.get('type', 'low_dim')\n",
    "        shape = attr.get('shape')\n",
    "        if type == 'rgb':\n",
    "            this_imgs_in = env_obs[key]\n",
    "            t,hi,wi,ci = this_imgs_in.shape\n",
    "            co,ho,wo = shape\n",
    "            assert ci == co\n",
    "            out_imgs = this_imgs_in\n",
    "            if (ho != hi) or (wo != wi) or (this_imgs_in.dtype == np.uint8):\n",
    "                tf = get_image_transform(\n",
    "                    input_res=(wi,hi), \n",
    "                    output_res=(wo,ho), \n",
    "                    bgr_to_rgb=False)\n",
    "                out_imgs = np.stack([tf(x) for x in this_imgs_in])\n",
    "                if this_imgs_in.dtype == np.uint8:\n",
    "                    out_imgs = out_imgs.astype(np.float32) / 255\n",
    "            # THWC to TCHW\n",
    "            obs_dict_np[key] = np.moveaxis(out_imgs,-1,1)\n",
    "        elif type == 'low_dim':\n",
    "            this_data_in = env_obs[key]\n",
    "            if 'pose' in key and shape == (2,):\n",
    "                # take X,Y coordinates\n",
    "                this_data_in = this_data_in[...,[0,1]]\n",
    "            obs_dict_np[key] = this_data_in\n",
    "    return obs_dict_np\n",
    "\n",
    "\n",
    "def get_real_obs_resolution(\n",
    "        shape_meta: dict\n",
    "        ) -> Tuple[int, int]:\n",
    "    out_res = None\n",
    "    obs_shape_meta = shape_meta['obs']\n",
    "    for key, attr in obs_shape_meta.items():\n",
    "        type = attr.get('type', 'low_dim')\n",
    "        shape = attr.get('shape')\n",
    "        if type == 'rgb':\n",
    "            co,ho,wo = shape\n",
    "            if out_res is None:\n",
    "                out_res = (wo, ho)\n",
    "            assert out_res == (wo, ho)\n",
    "    return out_res\n",
    "\n",
    "# define a fake env \n",
    "DEFAULT_OBS_KEY_MAP = {\n",
    "    # robot\n",
    "    'ActualTCPPose': 'robot_eef_pose',\n",
    "    'ActualTCPSpeed': 'robot_eef_pose_vel',\n",
    "    'ActualQ': 'robot_joint',\n",
    "    'ActualQd': 'robot_joint_vel',\n",
    "    # timestamps\n",
    "    'step_idx': 'step_idx',\n",
    "    'timestamp': 'timestamp'\n",
    "}\n",
    "class Env:\n",
    "    def __init__(self,\n",
    "            # required params\n",
    "            output_dir,\n",
    "            kuavo_robot_instance,\n",
    "            \n",
    "            # env params\n",
    "            frequency=10,\n",
    "            n_obs_steps=2,\n",
    "            \n",
    "            # obs\n",
    "            obs_image_resolution=(640, 480),\n",
    "            max_obs_buffer_size=30,\n",
    "            camera_serial_number=None,\n",
    "            obs_key_map=DEFAULT_OBS_KEY_MAP,\n",
    "            obs_float32=False,\n",
    "            \n",
    "            # action\n",
    "            max_pos_speed=0.25,\n",
    "            max_rot_speed=0.6,\n",
    "            # robot\n",
    "            tcp_offset=0.13,\n",
    "            init_joints=False,\n",
    "            \n",
    "            # video capture params\n",
    "            video_capture_fps=30,\n",
    "            video_capture_resolution=(1280,720),\n",
    "            \n",
    "            ### ???\n",
    "            # saving params\n",
    "            record_raw_video=True,\n",
    "            thread_per_video=2,\n",
    "            video_crf=21,\n",
    "            # vis params\n",
    "            enable_multi_cam_vis=True,\n",
    "            multi_cam_vis_resolution=(1280,720),\n",
    "            ### ???\n",
    "            # shared memory \n",
    "            shm_manager=None\n",
    "    ):\n",
    "        print(\"Env init\")  \n",
    "        self.output_dir = output_dir\n",
    "        self.kuavo_robot_instance = kuavo_robot_instance\n",
    "        self.frequency = frequency\n",
    "        self.n_obs_steps = n_obs_steps\n",
    "        self.obs_image_resolution = obs_image_resolution\n",
    "        self.max_obs_buffer_size = max_obs_buffer_size\n",
    "        self.camera_serial_number = camera_serial_number\n",
    "        self.obs_key_map = obs_key_map\n",
    "        self.obs_float32 = obs_float32\n",
    "        self.max_pos_speed = max_pos_speed\n",
    "        self.max_rot_speed = max_rot_speed\n",
    "        self.tcp_offset = tcp_offset\n",
    "        self.init_joints = init_joints\n",
    "        self.video_capture_fps = video_capture_fps\n",
    "        self.video_capture_resolution = video_capture_resolution\n",
    "        self.record_raw_video = record_raw_video\n",
    "        self.thread_per_video = thread_per_video\n",
    "        self.video_crf = video_crf\n",
    "        self.enable_multi_cam_vis = enable_multi_cam_vis\n",
    "        self.multi_cam_vis_resolution = multi_cam_vis_resolution\n",
    "        self.shm_manager = shm_manager\n",
    "\n",
    "\n",
    "    def get_fake_obs_dict(self):\n",
    "        obs = dict()\n",
    "        obs['anent_pos'] = np.random.rand(2)\n",
    "        obs['img'] = np.random.rand(3,640,480)\n",
    "        return obs\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env init\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'anent_pos': array([0.37454012, 0.95071431]),\n",
       " 'img': array([[[7.31993942e-01, 5.98658484e-01, 1.56018640e-01, ...,\n",
       "          5.23097844e-01, 6.29398638e-01, 6.95748689e-01],\n",
       "         [4.54541065e-01, 6.27558080e-01, 5.84314312e-01, ...,\n",
       "          7.38033616e-01, 8.27986679e-02, 6.03152109e-01],\n",
       "         [2.45349110e-01, 3.89295614e-01, 2.88693737e-01, ...,\n",
       "          6.73432433e-01, 9.69912046e-01, 9.39007158e-02],\n",
       "         ...,\n",
       "         [5.58969363e-01, 5.76026859e-01, 9.01943320e-01, ...,\n",
       "          7.40889624e-01, 7.82210445e-01, 7.57657125e-01],\n",
       "         [4.31034703e-01, 7.47578759e-02, 2.99743171e-01, ...,\n",
       "          7.43603380e-01, 2.35576933e-01, 9.83953269e-01],\n",
       "         [2.12456817e-01, 1.93842413e-01, 5.66702916e-01, ...,\n",
       "          9.09252102e-01, 7.72346211e-01, 6.08759915e-01]],\n",
       " \n",
       "        [[2.06682862e-01, 7.90432806e-01, 1.97867048e-01, ...,\n",
       "          3.72312954e-02, 3.23367946e-01, 2.65602402e-01],\n",
       "         [4.53094478e-01, 6.30305867e-01, 4.29000558e-01, ...,\n",
       "          6.93758971e-01, 3.58291079e-01, 7.51449265e-01],\n",
       "         [8.02095501e-01, 3.02656214e-01, 9.28692313e-01, ...,\n",
       "          3.84026596e-01, 5.84464029e-01, 2.51945740e-01],\n",
       "         ...,\n",
       "         [9.50073191e-01, 9.02213742e-01, 3.51483950e-01, ...,\n",
       "          2.80539676e-01, 8.69505330e-01, 6.01282281e-01],\n",
       "         [4.74570025e-01, 6.77823201e-01, 3.48637941e-01, ...,\n",
       "          8.84671238e-02, 1.52944866e-01, 5.61488449e-01],\n",
       "         [7.43118426e-01, 2.33645996e-01, 1.54275298e-01, ...,\n",
       "          9.88511879e-01, 5.38270573e-01, 9.79357340e-01]],\n",
       " \n",
       "        [[9.99111218e-01, 3.50279411e-01, 6.27914753e-04, ...,\n",
       "          8.32974734e-01, 8.05758794e-01, 7.18599933e-01],\n",
       "         [4.69808380e-01, 5.36298314e-01, 8.48497114e-01, ...,\n",
       "          6.33093387e-01, 6.67300984e-01, 7.77016815e-01],\n",
       "         [5.81990639e-01, 2.17385310e-01, 4.89996886e-01, ...,\n",
       "          7.01220894e-01, 7.14040840e-01, 9.17078253e-02],\n",
       "         ...,\n",
       "         [1.00669305e-01, 8.27989221e-01, 5.90485295e-01, ...,\n",
       "          5.55672492e-01, 2.00110802e-01, 4.42178457e-01],\n",
       "         [4.80664261e-01, 5.38192596e-01, 8.04459664e-01, ...,\n",
       "          2.30632085e-01, 8.47770578e-01, 9.49469824e-01],\n",
       "         [7.10033886e-02, 9.62960052e-01, 3.29292452e-01, ...,\n",
       "          7.42751774e-01, 9.83114390e-01, 2.16848098e-01]]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env:Env = Env(output, None, frequency=frequency, n_obs_steps=n_obs_steps, obs_image_resolution=obs_res)\n",
    "env.get_fake_obs_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== policy control loop ==============\n",
    "try:\n",
    "    # start episode\n",
    "    policy.reset()\n",
    "    start_delay = 1.0\n",
    "    eval_t_start = time.time() + start_delay\n",
    "    t_start = time.monotonic() + start_delay\n",
    "    env.start_episode(eval_t_start)\n",
    "    # wait for 1/30 sec to get the closest frame actually\n",
    "    # reduces overall latency\n",
    "    frame_latency = 1/30\n",
    "    precise_wait(eval_t_start - frame_latency, time_func=time.time)\n",
    "    print(\"Started!\")\n",
    "    iter_idx = 0\n",
    "    term_area_start_timestamp = float('inf')\n",
    "    perv_target_pose = None\n",
    "    while True:\n",
    "        # calculate timing\n",
    "        t_cycle_end = t_start + (iter_idx + steps_per_inference) * dt\n",
    "\n",
    "        # get obs\n",
    "        print('get_obs')\n",
    "        obs = env.get_obs()\n",
    "        obs_timestamps = obs['timestamp']\n",
    "        print(f'Obs latency {time.time() - obs_timestamps[-1]}')\n",
    "\n",
    "        # run inference\n",
    "        with torch.no_grad():\n",
    "            s = time.time()\n",
    "            obs_dict_np = get_real_obs_dict(\n",
    "                env_obs=obs, shape_meta=cfg.task.shape_meta)\n",
    "            obs_dict = dict_apply(obs_dict_np, \n",
    "                lambda x: torch.from_numpy(x).unsqueeze(0).to(device))\n",
    "            result = policy.predict_action(obs_dict)\n",
    "            # this action starts from the first obs step\n",
    "            action = result['action'][0].detach().to('cpu').numpy()\n",
    "            print('Inference latency:', time.time() - s)\n",
    "        \n",
    "        # convert policy action to env actions\n",
    "        if delta_action:\n",
    "            assert len(action) == 1\n",
    "            if perv_target_pose is None:\n",
    "                perv_target_pose = obs['robot_eef_pose'][-1]\n",
    "            this_target_pose = perv_target_pose.copy()\n",
    "            this_target_pose[[0,1]] += action[-1]\n",
    "            perv_target_pose = this_target_pose\n",
    "            this_target_poses = np.expand_dims(this_target_pose, axis=0)\n",
    "        else:\n",
    "            this_target_poses = np.zeros((len(action), len(target_pose)), dtype=np.float64)\n",
    "            this_target_poses[:] = target_pose\n",
    "            this_target_poses[:,[0,1]] = action\n",
    "\n",
    "        # deal with timing\n",
    "        # the same step actions are always the target for\n",
    "        action_timestamps = (np.arange(len(action), dtype=np.float64) + action_offset\n",
    "            ) * dt + obs_timestamps[-1]\n",
    "        action_exec_latency = 0.01\n",
    "        curr_time = time.time()\n",
    "        is_new = action_timestamps > (curr_time + action_exec_latency)\n",
    "        if np.sum(is_new) == 0:\n",
    "            # exceeded time budget, still do something\n",
    "            this_target_poses = this_target_poses[[-1]]\n",
    "            # schedule on next available step\n",
    "            next_step_idx = int(np.ceil((curr_time - eval_t_start) / dt))\n",
    "            action_timestamp = eval_t_start + (next_step_idx) * dt\n",
    "            print('Over budget', action_timestamp - curr_time)\n",
    "            action_timestamps = np.array([action_timestamp])\n",
    "        else:\n",
    "            this_target_poses = this_target_poses[is_new]\n",
    "            action_timestamps = action_timestamps[is_new]\n",
    "\n",
    "        # clip actions\n",
    "        this_target_poses[:,:2] = np.clip(\n",
    "            this_target_poses[:,:2], [0.25, -0.45], [0.77, 0.40])\n",
    "\n",
    "        # execute actions\n",
    "        env.exec_actions(\n",
    "            actions=this_target_poses,\n",
    "            timestamps=action_timestamps\n",
    "        )\n",
    "        print(f\"Submitted {len(this_target_poses)} steps of actions.\")\n",
    "\n",
    "        # visualize\n",
    "        episode_id = env.replay_buffer.n_episodes\n",
    "        vis_img = obs[f'camera_{vis_camera_idx}'][-1]\n",
    "        text = 'Episode: {}, Time: {:.1f}'.format(\n",
    "            episode_id, time.monotonic() - t_start\n",
    "        )\n",
    "        cv2.putText(\n",
    "            vis_img,\n",
    "            text,\n",
    "            (10,20),\n",
    "            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=0.5,\n",
    "            thickness=1,\n",
    "            color=(255,255,255)\n",
    "        )\n",
    "        cv2.imshow('default', vis_img[...,::-1])\n",
    "\n",
    "\n",
    "        key_stroke = cv2.pollKey()\n",
    "        if key_stroke == ord('s'):\n",
    "            # Stop episode\n",
    "            # Hand control back to human\n",
    "            env.end_episode()\n",
    "            print('Stopped.')\n",
    "            break\n",
    "\n",
    "        # auto termination\n",
    "        terminate = False\n",
    "        if time.monotonic() - t_start > max_duration:\n",
    "            terminate = True\n",
    "            print('Terminated by the timeout!')\n",
    "\n",
    "        term_pose = np.array([ 3.40948500e-01,  2.17721816e-01,  4.59076878e-02,  2.22014183e+00, -2.22184883e+00, -4.07186655e-04])\n",
    "        curr_pose = obs['robot_eef_pose'][-1]\n",
    "        dist = np.linalg.norm((curr_pose - term_pose)[:2], axis=-1)\n",
    "        if dist < 0.03:\n",
    "            # in termination area\n",
    "            curr_timestamp = obs['timestamp'][-1]\n",
    "            if term_area_start_timestamp > curr_timestamp:\n",
    "                term_area_start_timestamp = curr_timestamp\n",
    "            else:\n",
    "                term_area_time = curr_timestamp - term_area_start_timestamp\n",
    "                if term_area_time > 0.5:\n",
    "                    terminate = True\n",
    "                    print('Terminated by the policy!')\n",
    "        else:\n",
    "            # out of the area\n",
    "            term_area_start_timestamp = float('inf')\n",
    "\n",
    "        if terminate:\n",
    "            env.end_episode()\n",
    "            break\n",
    "\n",
    "        # wait for execution\n",
    "        precise_wait(t_cycle_end - frame_latency)\n",
    "        iter_idx += steps_per_inference\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted!\")\n",
    "    # stop robot.\n",
    "    env.end_episode()\n",
    "\n",
    "print(\"Stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
