{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///\n",
      "\u001b[31mERROR: file:/// does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"/app/data/outputs/2024.09.22/07.35.36_train_diffusion_unet_hybrid_pusht_image/checkpoints/latest.ckpt\"\n",
    "output=\"/app/data/outputs/2024.09.22/07.35.36_train_diffusion_unet_hybrid_pusht_image/checkpoints/output\"\n",
    "robot_ip=\"192.168.0.204\"\n",
    "match_dataset=\"/app/data/pusht_real/real_pusht_20230105\"\n",
    "match_episode=None\n",
    "vis_camera_idx=0\n",
    "init_joints=False\n",
    "steps_per_inference=6\n",
    "max_duration=60\n",
    "frequency=10\n",
    "command_latency=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/opt/ros/noetic/lib/python3/dist-packages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source /app/diffusion_policy/real_world/dynamic_biped_ws/devel/setup.zsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospkg\n",
    "import rospy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dynamic_biped'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momegaconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OmegaConf\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion_policy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreal_world\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreal_env_kuavo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KuavoEnv\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# from diffusion_policy.real_world.spacemouse_shared_memory import Spacemouse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion_policy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecise_sleep\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precise_wait\n",
      "File \u001b[0;32m/app/diffusion_policy/real_world/real_env_kuavo.py:34\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rotation \u001b[38;5;28;01mas\u001b[39;00m R\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Union, Dict, Callable\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdynamic_biped\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmsg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m robotArmInfo, recordArmHandPose, robotHandPosition\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdynamic_biped\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m controlEndHand, controlEndHandRequest\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dynamic_biped'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from multiprocessing.managers import SharedMemoryManager\n",
    "import click\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import dill\n",
    "import hydra\n",
    "import pathlib\n",
    "import skvideo.io\n",
    "from omegaconf import OmegaConf\n",
    "import scipy.spatial.transform as st\n",
    "from diffusion_policy.real_world.real_env_kuavo import KuavoEnv\n",
    "# from diffusion_policy.real_world.spacemouse_shared_memory import Spacemouse\n",
    "from diffusion_policy.common.precise_sleep import precise_wait\n",
    "from diffusion_policy.real_world.real_inference_util import (\n",
    "    get_real_obs_resolution, \n",
    "    get_real_obs_dict)\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.policy.base_image_policy import BaseImagePolicy\n",
    "from diffusion_policy.common.cv2_util import get_image_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded initial frame for 1 episodes\n"
     ]
    }
   ],
   "source": [
    "# load match_dataset\n",
    "match_camera_idx = 0\n",
    "episode_first_frame_map = dict()\n",
    "if match_dataset is not None:\n",
    "    match_dir = pathlib.Path(match_dataset)\n",
    "    match_video_dir = match_dir.joinpath('videos')\n",
    "    for vid_dir in match_video_dir.glob(\"*/\"):\n",
    "        episode_idx = int(vid_dir.stem)\n",
    "        match_video_path = vid_dir.joinpath(f'{match_camera_idx}.mp4')\n",
    "        if match_video_path.exists():\n",
    "            frames = skvideo.io.vread(\n",
    "                str(match_video_path), num_frames=1)\n",
    "            episode_first_frame_map[episode_idx] = frames[0]\n",
    "        break\n",
    "print(f\"Loaded initial frame for {len(episode_first_frame_map)} episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['agent_pos']\n",
      "using obs modality: rgb with keys: ['image']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/robodiff/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/robodiff/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusion params: 6.502567e+07\n",
      "Vision params: 1.119709e+07\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "ckpt_path = input\n",
    "payload = torch.load(open(ckpt_path, 'rb'), pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "cls = hydra.utils.get_class(cfg._target_)\n",
    "workspace = cls(cfg)\n",
    "workspace: BaseWorkspace\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacks for method-specific setup.\n",
    "action_offset = 0\n",
    "delta_action = False\n",
    "if 'diffusion' in cfg.name:\n",
    "    # diffusion model\n",
    "    policy: BaseImagePolicy\n",
    "    policy = workspace.model\n",
    "    if cfg.training.use_ema:\n",
    "        policy = workspace.ema_model\n",
    "    device = torch.device('cuda')\n",
    "    policy.eval().to(device)\n",
    "    # set inference params\n",
    "    policy.num_inference_steps = 16 # DDIM inference iterations\n",
    "    policy.n_action_steps = policy.horizon - policy.n_obs_steps + 1\n",
    "elif 'robomimic' in cfg.name:\n",
    "    # BCRNN model\n",
    "    policy: BaseImagePolicy\n",
    "    policy = workspace.model\n",
    "    device = torch.device('cuda')\n",
    "    policy.eval().to(device)\n",
    "    # BCRNN always has action horizon of 1\n",
    "    steps_per_inference = 1\n",
    "    action_offset = cfg.n_latency_steps\n",
    "    delta_action = cfg.task.dataset.get('delta_action', False)\n",
    "elif 'ibc' in cfg.name:\n",
    "    policy: BaseImagePolicy\n",
    "    policy = workspace.model\n",
    "    policy.pred_n_iter = 5\n",
    "    policy.pred_n_samples = 4096\n",
    "    device = torch.device('cuda')\n",
    "    policy.eval().to(device)\n",
    "    steps_per_inference = 1\n",
    "    action_offset = 1\n",
    "    delta_action = cfg.task.dataset.get('delta_action', False)\n",
    "else:\n",
    "    raise RuntimeError(\"Unsupported policy type: \", cfg.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_obs_steps:  2\n",
      "steps_per_inference: 6\n",
      "action_offset: 0\n"
     ]
    }
   ],
   "source": [
    "# setup experiment\n",
    "dt = 1/frequency\n",
    "\n",
    "obs_res = get_real_obs_resolution(cfg.task.shape_meta)\n",
    "n_obs_steps = cfg.n_obs_steps\n",
    "print(\"n_obs_steps: \", n_obs_steps)\n",
    "print(\"steps_per_inference:\", steps_per_inference)\n",
    "print(\"action_offset:\", action_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KuavoEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mKuavoEnv\u001b[49m(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput,\n\u001b[1;32m      3\u001b[0m     frequency\u001b[38;5;241m=\u001b[39mfrequency,\n\u001b[1;32m      4\u001b[0m     n_obs_steps\u001b[38;5;241m=\u001b[39mn_obs_steps,\n\u001b[1;32m      5\u001b[0m     obs_image_resolution\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m480\u001b[39m),\n\u001b[1;32m      6\u001b[0m     max_obs_buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \n\u001b[1;32m      8\u001b[0m     robot_publish_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m125\u001b[39m,\n\u001b[1;32m      9\u001b[0m     video_capture_fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m     10\u001b[0m     video_capture_resolution\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1280\u001b[39m, \u001b[38;5;241m720\u001b[39m),\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KuavoEnv' is not defined"
     ]
    }
   ],
   "source": [
    "env = KuavoEnv(\n",
    "    output_dir=output,\n",
    "    frequency=frequency,\n",
    "    n_obs_steps=n_obs_steps,\n",
    "    obs_image_resolution=(640, 480),\n",
    "    max_obs_buffer_size=30,\n",
    "    \n",
    "    robot_publish_rate=125,\n",
    "    video_capture_fps=30,\n",
    "    video_capture_resolution=(1280, 720),\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env init\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'anent_pos': array([0.37454012, 0.95071431]),\n",
       " 'img': array([[[7.31993942e-01, 5.98658484e-01, 1.56018640e-01, ...,\n",
       "          5.23097844e-01, 6.29398638e-01, 6.95748689e-01],\n",
       "         [4.54541065e-01, 6.27558080e-01, 5.84314312e-01, ...,\n",
       "          7.38033616e-01, 8.27986679e-02, 6.03152109e-01],\n",
       "         [2.45349110e-01, 3.89295614e-01, 2.88693737e-01, ...,\n",
       "          6.73432433e-01, 9.69912046e-01, 9.39007158e-02],\n",
       "         ...,\n",
       "         [5.58969363e-01, 5.76026859e-01, 9.01943320e-01, ...,\n",
       "          7.40889624e-01, 7.82210445e-01, 7.57657125e-01],\n",
       "         [4.31034703e-01, 7.47578759e-02, 2.99743171e-01, ...,\n",
       "          7.43603380e-01, 2.35576933e-01, 9.83953269e-01],\n",
       "         [2.12456817e-01, 1.93842413e-01, 5.66702916e-01, ...,\n",
       "          9.09252102e-01, 7.72346211e-01, 6.08759915e-01]],\n",
       " \n",
       "        [[2.06682862e-01, 7.90432806e-01, 1.97867048e-01, ...,\n",
       "          3.72312954e-02, 3.23367946e-01, 2.65602402e-01],\n",
       "         [4.53094478e-01, 6.30305867e-01, 4.29000558e-01, ...,\n",
       "          6.93758971e-01, 3.58291079e-01, 7.51449265e-01],\n",
       "         [8.02095501e-01, 3.02656214e-01, 9.28692313e-01, ...,\n",
       "          3.84026596e-01, 5.84464029e-01, 2.51945740e-01],\n",
       "         ...,\n",
       "         [9.50073191e-01, 9.02213742e-01, 3.51483950e-01, ...,\n",
       "          2.80539676e-01, 8.69505330e-01, 6.01282281e-01],\n",
       "         [4.74570025e-01, 6.77823201e-01, 3.48637941e-01, ...,\n",
       "          8.84671238e-02, 1.52944866e-01, 5.61488449e-01],\n",
       "         [7.43118426e-01, 2.33645996e-01, 1.54275298e-01, ...,\n",
       "          9.88511879e-01, 5.38270573e-01, 9.79357340e-01]],\n",
       " \n",
       "        [[9.99111218e-01, 3.50279411e-01, 6.27914753e-04, ...,\n",
       "          8.32974734e-01, 8.05758794e-01, 7.18599933e-01],\n",
       "         [4.69808380e-01, 5.36298314e-01, 8.48497114e-01, ...,\n",
       "          6.33093387e-01, 6.67300984e-01, 7.77016815e-01],\n",
       "         [5.81990639e-01, 2.17385310e-01, 4.89996886e-01, ...,\n",
       "          7.01220894e-01, 7.14040840e-01, 9.17078253e-02],\n",
       "         ...,\n",
       "         [1.00669305e-01, 8.27989221e-01, 5.90485295e-01, ...,\n",
       "          5.55672492e-01, 2.00110802e-01, 4.42178457e-01],\n",
       "         [4.80664261e-01, 5.38192596e-01, 8.04459664e-01, ...,\n",
       "          2.30632085e-01, 8.47770578e-01, 9.49469824e-01],\n",
       "         [7.10033886e-02, 9.62960052e-01, 3.29292452e-01, ...,\n",
       "          7.42751774e-01, 9.83114390e-01, 2.16848098e-01]]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env:Env = Env(output, None, frequency=frequency, n_obs_steps=n_obs_steps, obs_image_resolution=obs_res)\n",
    "env.get_fake_obs_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== policy control loop ==============\n",
    "try:\n",
    "    # start episode\n",
    "    policy.reset()\n",
    "    start_delay = 1.0\n",
    "    eval_t_start = time.time() + start_delay\n",
    "    t_start = time.monotonic() + start_delay\n",
    "    env.start_episode(eval_t_start)\n",
    "    # wait for 1/30 sec to get the closest frame actually\n",
    "    # reduces overall latency\n",
    "    frame_latency = 1/30\n",
    "    precise_wait(eval_t_start - frame_latency, time_func=time.time)\n",
    "    print(\"Started!\")\n",
    "    iter_idx = 0\n",
    "    term_area_start_timestamp = float('inf')\n",
    "    perv_target_pose = None\n",
    "    while True:\n",
    "        # calculate timing\n",
    "        t_cycle_end = t_start + (iter_idx + steps_per_inference) * dt\n",
    "\n",
    "        # get obs\n",
    "        print('get_obs')\n",
    "        obs = env.get_obs()\n",
    "        obs_timestamps = obs['timestamp']\n",
    "        print(f'Obs latency {time.time() - obs_timestamps[-1]}')\n",
    "\n",
    "        # run inference\n",
    "        with torch.no_grad():\n",
    "            s = time.time()\n",
    "            obs_dict_np = get_real_obs_dict(\n",
    "                env_obs=obs, shape_meta=cfg.task.shape_meta)\n",
    "            obs_dict = dict_apply(obs_dict_np, \n",
    "                lambda x: torch.from_numpy(x).unsqueeze(0).to(device))\n",
    "            result = policy.predict_action(obs_dict)\n",
    "            # this action starts from the first obs step\n",
    "            action = result['action'][0].detach().to('cpu').numpy()\n",
    "            print('Inference latency:', time.time() - s)\n",
    "        \n",
    "        # convert policy action to env actions\n",
    "        if delta_action:\n",
    "            assert len(action) == 1\n",
    "            if perv_target_pose is None:\n",
    "                perv_target_pose = obs['robot_eef_pose'][-1]\n",
    "            this_target_pose = perv_target_pose.copy()\n",
    "            this_target_pose[[0,1]] += action[-1]\n",
    "            perv_target_pose = this_target_pose\n",
    "            this_target_poses = np.expand_dims(this_target_pose, axis=0)\n",
    "        else:\n",
    "            this_target_poses = np.zeros((len(action), len(target_pose)), dtype=np.float64)\n",
    "            this_target_poses[:] = target_pose\n",
    "            this_target_poses[:,[0,1]] = action\n",
    "\n",
    "        # deal with timing\n",
    "        # the same step actions are always the target for\n",
    "        action_timestamps = (np.arange(len(action), dtype=np.float64) + action_offset\n",
    "            ) * dt + obs_timestamps[-1]\n",
    "        action_exec_latency = 0.01\n",
    "        curr_time = time.time()\n",
    "        is_new = action_timestamps > (curr_time + action_exec_latency)\n",
    "        if np.sum(is_new) == 0:\n",
    "            # exceeded time budget, still do something\n",
    "            this_target_poses = this_target_poses[[-1]]\n",
    "            # schedule on next available step\n",
    "            next_step_idx = int(np.ceil((curr_time - eval_t_start) / dt))\n",
    "            action_timestamp = eval_t_start + (next_step_idx) * dt\n",
    "            print('Over budget', action_timestamp - curr_time)\n",
    "            action_timestamps = np.array([action_timestamp])\n",
    "        else:\n",
    "            this_target_poses = this_target_poses[is_new]\n",
    "            action_timestamps = action_timestamps[is_new]\n",
    "\n",
    "        # clip actions\n",
    "        this_target_poses[:,:2] = np.clip(\n",
    "            this_target_poses[:,:2], [0.25, -0.45], [0.77, 0.40])\n",
    "\n",
    "        # execute actions\n",
    "        env.exec_actions(\n",
    "            actions=this_target_poses,\n",
    "            timestamps=action_timestamps\n",
    "        )\n",
    "        print(f\"Submitted {len(this_target_poses)} steps of actions.\")\n",
    "\n",
    "        # visualize\n",
    "        episode_id = env.replay_buffer.n_episodes\n",
    "        vis_img = obs[f'camera_{vis_camera_idx}'][-1]\n",
    "        text = 'Episode: {}, Time: {:.1f}'.format(\n",
    "            episode_id, time.monotonic() - t_start\n",
    "        )\n",
    "        cv2.putText(\n",
    "            vis_img,\n",
    "            text,\n",
    "            (10,20),\n",
    "            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=0.5,\n",
    "            thickness=1,\n",
    "            color=(255,255,255)\n",
    "        )\n",
    "        cv2.imshow('default', vis_img[...,::-1])\n",
    "\n",
    "\n",
    "        key_stroke = cv2.pollKey()\n",
    "        if key_stroke == ord('s'):\n",
    "            # Stop episode\n",
    "            # Hand control back to human\n",
    "            env.end_episode()\n",
    "            print('Stopped.')\n",
    "            break\n",
    "\n",
    "        # auto termination\n",
    "        terminate = False\n",
    "        if time.monotonic() - t_start > max_duration:\n",
    "            terminate = True\n",
    "            print('Terminated by the timeout!')\n",
    "\n",
    "        term_pose = np.array([ 3.40948500e-01,  2.17721816e-01,  4.59076878e-02,  2.22014183e+00, -2.22184883e+00, -4.07186655e-04])\n",
    "        curr_pose = obs['robot_eef_pose'][-1]\n",
    "        dist = np.linalg.norm((curr_pose - term_pose)[:2], axis=-1)\n",
    "        if dist < 0.03:\n",
    "            # in termination area\n",
    "            curr_timestamp = obs['timestamp'][-1]\n",
    "            if term_area_start_timestamp > curr_timestamp:\n",
    "                term_area_start_timestamp = curr_timestamp\n",
    "            else:\n",
    "                term_area_time = curr_timestamp - term_area_start_timestamp\n",
    "                if term_area_time > 0.5:\n",
    "                    terminate = True\n",
    "                    print('Terminated by the policy!')\n",
    "        else:\n",
    "            # out of the area\n",
    "            term_area_start_timestamp = float('inf')\n",
    "\n",
    "        if terminate:\n",
    "            env.end_episode()\n",
    "            break\n",
    "\n",
    "        # wait for execution\n",
    "        precise_wait(t_cycle_end - frame_latency)\n",
    "        iter_idx += steps_per_inference\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted!\")\n",
    "    # stop robot.\n",
    "    env.end_episode()\n",
    "\n",
    "print(\"Stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
